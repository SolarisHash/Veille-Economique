#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Correctif pour √©liminer les doublons et valider les URLs
√Ä int√©grer dans votre g√©n√©rateur de rapports
"""

import requests
from urllib.parse import urlparse
from typing import List, Dict, Set
import time
import re
from collections import defaultdict

class ReportFixer:
    """Correcteur de rapports - Anti-doublons et validation URLs"""
    
    def __init__(self):
        self.urls_valides = {}  # Cache des URLs valid√©es
        self.urls_invalides = set()  # URLs √† √©viter
        
    def deduplicate_enterprises(self, entreprises_enrichies: List[Dict]) -> List[Dict]:
        """√âlimine les doublons d'entreprises"""
        print("üîÑ D√©duplication des entreprises...")
        
        entreprises_uniques = {}  # Cl√©: SIRET, Valeur: entreprise
        entreprises_deduplicates = []
        
        for entreprise in entreprises_enrichies:
            siret = entreprise.get('siret', '')
            nom = entreprise.get('nom', '')
            
            # Cl√© unique bas√©e sur SIRET + nom
            cle_unique = f"{siret}_{nom}".strip('_')
            
            if cle_unique in entreprises_uniques:
                # Fusion des donn√©es si doublon d√©tect√©
                print(f"   üîÑ Doublon d√©tect√©: {nom[:50]}...")
                entreprise_existante = entreprises_uniques[cle_unique]
                entreprise_fusionnee = self._fusionner_entreprises(entreprise_existante, entreprise)
                entreprises_uniques[cle_unique] = entreprise_fusionnee
            else:
                entreprises_uniques[cle_unique] = entreprise
                print(f"   ‚úÖ Unique: {nom[:50]}...")
        
        entreprises_deduplicates = list(entreprises_uniques.values())
        
        print(f"üìä D√©duplication termin√©e:")
        print(f"   üì• Entreprises avant: {len(entreprises_enrichies)}")
        print(f"   üì§ Entreprises apr√®s: {len(entreprises_deduplicates)}")
        print(f"   üóëÔ∏è Doublons √©limin√©s: {len(entreprises_enrichies) - len(entreprises_deduplicates)}")
        
        return entreprises_deduplicates
    
    def _fusionner_entreprises(self, entreprise1: Dict, entreprise2: Dict) -> Dict:
        """Fusionne deux entreprises dupliqu√©es en conservant le meilleur"""
        print(f"     üîó Fusion en cours...")
        
        # Prendre l'entreprise avec le meilleur score comme base
        score1 = entreprise1.get('score_global', 0)
        score2 = entreprise2.get('score_global', 0)
        
        if score2 > score1:
            entreprise_base = entreprise2.copy()
            entreprise_fusion = entreprise1
        else:
            entreprise_base = entreprise1.copy()
            entreprise_fusion = entreprise2
        
        # Fusionner les th√©matiques
        analyse1 = entreprise1.get('analyse_thematique', {})
        analyse2 = entreprise2.get('analyse_thematique', {})
        
        # Merger les analyses th√©matiques
        for thematique, data in analyse2.items():
            if data.get('trouve', False):
                if thematique not in analyse1 or not analyse1[thematique].get('trouve', False):
                    # Ajouter la th√©matique manquante
                    analyse1[thematique] = data
                else:
                    # Fusionner les d√©tails
                    details1 = analyse1[thematique].get('details', [])
                    details2 = data.get('details', [])
                    analyse1[thematique]['details'] = details1 + details2
                    
                    # Prendre le meilleur score
                    score_max = max(
                        analyse1[thematique].get('score_pertinence', 0),
                        data.get('score_pertinence', 0)
                    )
                    analyse1[thematique]['score_pertinence'] = score_max
        
        entreprise_base['analyse_thematique'] = analyse1
        
        # Recalculer le score global
        entreprise_base['score_global'] = self._recalculer_score_global(analyse1)
        
        # Fusionner les th√©matiques principales
        themes1 = set(entreprise1.get('thematiques_principales', []))
        themes2 = set(entreprise2.get('thematiques_principales', []))
        entreprise_base['thematiques_principales'] = list(themes1.union(themes2))
        
        return entreprise_base
    
    def _recalculer_score_global(self, analyse_thematique: Dict) -> float:
        """Recalcule le score global apr√®s fusion"""
        scores_valides = [
            data['score_pertinence']
            for data in analyse_thematique.values()
            if data.get('trouve', False) and data.get('score_pertinence', 0) > 0.3
        ]
        
        if not scores_valides:
            return 0.0
        
        score_moyen = sum(scores_valides) / len(scores_valides)
        bonus_diversite = min(len(scores_valides) * 0.02, 0.1)
        
        return min(score_moyen + bonus_diversite, 0.8)
    
    def validate_and_fix_urls(self, entreprises: List[Dict]) -> List[Dict]:
        """Valide et corrige les URLs dans les rapports"""
        print("üîó Validation et correction des URLs...")
        
        total_urls = 0
        urls_corrigees = 0
        urls_supprimees = 0
        
        for entreprise in entreprises:
            nom_entreprise = entreprise.get('nom', 'N/A')
            print(f"   üîç URLs de: {nom_entreprise[:40]}...")
            
            analyse = entreprise.get('analyse_thematique', {})
            
            for thematique, data in analyse.items():
                if not data.get('trouve', False):
                    continue
                
                details = data.get('details', [])
                details_corriges = []
                
                for detail in details:
                    total_urls += 1
                    informations = detail.get('informations', {})
                    
                    # Traitement des extraits textuels avec URLs
                    extraits_textuels = informations.get('extraits_textuels', [])
                    extraits_corriges = []
                    
                    for extrait in extraits_textuels:
                        if isinstance(extrait, dict):
                            url = extrait.get('url', '')
                            
                            if url:
                                url_status = self._validate_url(url)
                                
                                if url_status == 'valid':
                                    extraits_corriges.append(extrait)
                                elif url_status == 'invalid':
                                    # Supprimer l'URL invalide mais garder le contenu
                                    extrait_sans_url = extrait.copy()
                                    extrait_sans_url['url'] = ''
                                    extrait_sans_url['url_status'] = 'removed_invalid'
                                    extraits_corriges.append(extrait_sans_url)
                                    urls_supprimees += 1
                                elif url_status == 'simulation':
                                    # Remplacer par une recherche Google
                                    extrait_corrige = self._fix_simulation_url(extrait, nom_entreprise, thematique)
                                    extraits_corriges.append(extrait_corrige)
                                    urls_corrigees += 1
                            else:
                                extraits_corriges.append(extrait)
                    
                    # Mettre √† jour les extraits corrig√©s
                    informations['extraits_textuels'] = extraits_corriges
                    details_corriges.append(detail)
                
                data['details'] = details_corriges
        
        print(f"üìä Validation URLs termin√©e:")
        print(f"   üì• URLs analys√©es: {total_urls}")
        print(f"   üîß URLs corrig√©es: {urls_corrigees}")
        print(f"   üóëÔ∏è URLs supprim√©es: {urls_supprimees}")
        
        return entreprises
    
    def _validate_url(self, url: str) -> str:
        """Valide une URL"""
        if not url or not url.startswith('http'):
            return 'invalid'
        
        # URLs de simulation d√©tect√©es
        simulation_indicators = [
            '-sante.fr/entreprises-locales',
            '-eco.fr/activites',
            '-news.fr/economie',
            '-formation.fr/services',
            'exemple-local.fr',
            'salon-lagny-sur-marne.fr'
        ]
        
        if any(indicator in url for indicator in simulation_indicators):
            return 'simulation'
        
        # Cache pour √©viter de tester la m√™me URL plusieurs fois
        if url in self.urls_valides:
            return self.urls_valides[url]
        
        if url in self.urls_invalides:
            return 'invalid'
        
        try:
            # Test rapide avec HEAD request
            response = requests.head(url, timeout=5, allow_redirects=True)
            
            if response.status_code < 400:
                self.urls_valides[url] = 'valid'
                return 'valid'
            else:
                self.urls_invalides.add(url)
                return 'invalid'
                
        except:
            self.urls_invalides.add(url)
            return 'invalid'
    
    def _fix_simulation_url(self, extrait: Dict, nom_entreprise: str, thematique: str) -> Dict:
        """Corrige une URL de simulation"""
        titre = extrait.get('titre', '')
        
        # G√©n√©rer une recherche Google appropri√©e
        search_query = f"{nom_entreprise} {thematique}".replace(' ', '+')
        google_url = f"https://www.google.fr/search?q={search_query}"
        
        extrait_corrige = extrait.copy()
        extrait_corrige.update({
            'url': google_url,
            'url_status': 'corrected_to_search',
            'original_url': extrait.get('url', ''),
            'correction_note': 'URL simul√©e remplac√©e par recherche Google'
        })
        
        return extrait_corrige

# INT√âGRATION DANS VOTRE G√âN√âRATEUR DE RAPPORTS
def integrate_report_fixer():
    """Code d'int√©gration dans generateur_rapports.py"""
    
    integration_code = '''
# Dans votre generateur_rapports.py, m√©thode generer_tous_rapports():

from report_fixer import ReportFixer  # Ajoutez cet import

class GenerateurRapports:
    def generer_tous_rapports(self, entreprises_enrichies: List[Dict]) -> Dict[str, str]:
        """Version avec correction automatique"""
        print("üìä G√©n√©ration de tous les rapports avec corrections")
        
        # ‚úÖ NOUVEAU: Correction automatique avant g√©n√©ration
        fixer = ReportFixer()
        
        # 1. √âlimination des doublons
        entreprises_uniques = fixer.deduplicate_enterprises(entreprises_enrichies)
        
        # 2. Validation et correction des URLs
        entreprises_corrigees = fixer.validate_and_fix_urls(entreprises_uniques)
        
        print(f"‚úÖ Corrections appliqu√©es:")
        print(f"   üóëÔ∏è Doublons √©limin√©s: {len(entreprises_enrichies) - len(entreprises_uniques)}")
        print(f"   üîó URLs corrig√©es automatiquement")
        
        # Continuer avec la g√©n√©ration normale des rapports
        rapports = {}
        
        # Vos rapports existants avec les donn√©es corrig√©es
        try:
            rapports['excel'] = self.generer_rapport_excel(entreprises_corrigees)
        except Exception as e:
            rapports['excel'] = f"ERREUR: {str(e)}"
        
        try:
            rapports['html'] = self.generer_rapport_html(entreprises_corrigees)
        except Exception as e:
            rapports['html'] = f"ERREUR: {str(e)}"
        
        # Autres rapports...
        
        return rapports
'''
    
    return integration_code

def _strip_html(s): return (s or '').strip()

def _fix_extraits_dupliques(html: str) -> str:
    """Supprime blocs d'extraits strictement identiques r√©p√©t√©s √† la suite."""
    # Heuristique : si trois <div ...> cons√©cutifs ont m√™me titre+desc+url -> n‚Äôen garder qu‚Äôun
    pattern = re.compile(
        r'(<div[^>]*>\s*<div[^>]*>\s*üåê[^<]*</div>\s*<div[^>]*>[^<]*</div>\s*(?:<div[^>]*>.*?</div>\s*)?</div>)(\s*\1)+',
        re.DOTALL | re.IGNORECASE
    )
    return re.sub(pattern, r'\1', html)

def _fix_noms_dupliques_commune(html: str) -> str:
    """Dans la carte 'R√©sum√© par Commune', supprime les r√©p√©titions imm√©diates 'ARGEDIS, ARGEDIS'."""
    # Simpliste mais efficace : remplacer ', NOM, NOM' par ', NOM'
    def dedup_list(match):
        bloc = match.group(1)
        noms = [n.strip() for n in bloc.split(',') if n.strip()]
        uniques = []
        for n in noms:
            if n not in uniques:
                uniques.append(n)
        return ', '.join(uniques)

    return re.sub(
        r'(<div style="font-size: 0\.9em; color: #2c3e50; line-height: 1\.4;">\s*)([^<]+)(\s*</div>)',
        lambda m: m.group(1) + dedup_list(m.group(2)) + m.group(3),
        html, flags=re.DOTALL
    )

def post_process_html(html: str) -> str:
    html = _fix_extraits_dupliques(html)
    html = _fix_noms_dupliques_commune(html)
    return html

def post_process_html(html: str) -> str:
    """
    Petit lissage HTML de fin de cha√Æne :
    - Supprime les doublons imm√©diats du type "ARGEDIS, ARGEDIS"
    - D√©duplique des suites "X, X, X"
    - Normalise espaces et virgules
    - √âvite les r√©p√©titions cons√©cutives de balises simples

    Le traitement reste volontairement conservateur (scope limit√© aux listes/phrases).
    """
    if not html:
        return html

    # 1) Nettoyage basique des espaces/virgules
    html = re.sub(r'\s+,', ', ', html)          # espace avant virgule -> apr√®s
    html = re.sub(r',\s+', ', ', html)          # normalise ",    " -> ", "
    html = re.sub(r'\s{2,}', ' ', html)         # espaces multiples -> simple espace

    # 2) D√©duplication de mots/expressions cons√©cutifs s√©par√©s par virgule
    #    Exemple: "ARGEDIS, ARGEDIS" -> "ARGEDIS"
    #    On reste prudent: on limite la longueur de l'expression et on √©vite de traverser tags/balises HTML.
    def dedupe_csv(match: re.Match) -> str:
        # prend la s√©quence captur√©e et supprime doublons cons√©cutifs
        items = [x.strip() for x in match.group(0).split(',')]
        deduped = []
        prev = None
        for it in items:
            if it != prev:
                deduped.append(it)
            prev = it
        return ', '.join(deduped)

    # cible les segments CSV "Mot[, Mot]..." sans chevrons (√©vite HTML tags)
    # On traite par blocs entre balises pour rester conservateur.
    def dedupe_in_text_segments(html_text: str) -> str:
        # Sur chaque bloc de texte (hors balises), on retire "X, X" imm√©diats
        # Passes multiples pour capturer des triples "X, X, X"
        for _ in range(2):
            html_text = re.sub(
                r'(?<![<>])\b([A-Z0-9][A-Z0-9\'&\-. ]{1,80}?)\b,\s+\1\b(?![^<]*>)',
                r'\1',
                html_text
            )
        return html_text

    html = dedupe_in_text_segments(html)

    # 3) D√©duplication douce dans les listes "bullet-like" s√©par√©es par " | "
    #    Exemple: "A | A | B" -> "A | B"
    def dedupe_pipe_segments(text: str) -> str:
        parts = text.split(' | ')
        result = []
        seen_prev = None
        for p in parts:
            if p != seen_prev:
                result.append(p)
            seen_prev = p
        return ' | '.join(result)

    html = re.sub(
        r'((?:[^<>]|<(?!/?(?:script|style)[^>]*>))+)',
        lambda m: dedupe_pipe_segments(m.group(1)),
        html,
        flags=re.IGNORECASE
    )

    # 4) √âvite la r√©p√©tition imm√©diate de m√™mes balises simples (ex: <div>..</div><div>..</div> identiques coll√©es)
    #    Ici on ne supprime pas, on ins√®re une fine espace pour √©viter "collage visuel" ; c‚Äôest safe.
    html = re.sub(r'(</(div|p)>\s*)(<(div|p)[ >])', r'\1\n\3', html, flags=re.IGNORECASE)

    # 5) Finitions ponctuation/espaces
    html = re.sub(r'\s+\.', '.', html)
    html = re.sub(r'\s+,', ', ', html)
    html = re.sub(r',\s+,', ', ', html)

    return html

if __name__ == "__main__":
    print("üß™ Test du correcteur de rapports")
    
    # Test d√©duplication
    fixer = ReportFixer()
    
    # Entreprises test avec doublons
    entreprises_test = [
        {
            'siret': '12345',
            'nom': 'ENTREPRISE TEST',
            'score_global': 0.5,
            'thematiques_principales': ['recrutements']
        },
        {
            'siret': '12345',  # M√™me SIRET = doublon
            'nom': 'ENTREPRISE TEST',
            'score_global': 0.3,
            'thematiques_principales': ['innovations']
        },
        {
            'siret': '67890',
            'nom': 'AUTRE ENTREPRISE',
            'score_global': 0.7,
            'thematiques_principales': ['evenements']
        }
    ]
    
    entreprises_uniques = fixer.deduplicate_enterprises(entreprises_test)
    
    print(f"\n‚úÖ Test d√©duplication:")
    print(f"   Avant: {len(entreprises_test)} entreprises")
    print(f"   Apr√®s: {len(entreprises_uniques)} entreprises")
    
    for ent in entreprises_uniques:
        print(f"   - {ent['nom']}: themes {ent['thematiques_principales']}")